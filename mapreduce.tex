\chapter{Extension}
\section{A Distributed Sort}
Many problems can be solved faster by parallelizing computation. At the extreme
end of the parallelization spectrum lies distributed computing, where
computation is distributed not among cores of a processor, but among individual
computers, each with their permanent storage, memory and processors.

MapReduce is model for distributing computation across many (hundredths or
thousands) of computers. This distribution often comes with a significant
overhead, and so it usually only pays off for data sets that are too large to
store on a single machine. It seems improbable that such a data set should exist
for movie ratings. The full \texttt{IMDB} data set consists of less than
100 billion ratings ($\sim 1.5 \times 10^6$, the most frequently rated of which
have $\sim 1.5 \times 10^6$ ratings each). A rating is a number
$\left(\left[1,10\right]\cap \mathbb{N}\right)$, which can be stored in
$\lceil \log(10)\rceil = 4$ bits, which means the full set of ratings can be
stored in less than $47$ GB. However, our \texttt{IMDB} motivation can be
directly translated to e.g. sensory network data, with the possible exception of
the small domain size, which we will not rely on in this section.

We describe a simple MapReduce algorithm that provides an exact solution to our
sorted-averages problem, and does so in a constant number of rounds. To achieve
constant-round sorting, we use the TeraSort algorithm \texttt{Owen O'Mally
(2008)}\citep{terabytesort} \texttt{Yufei Tao et
al.(2013)}\citep{minimalmapreduce}.

\subsection{MapReduce}
\label{sec:mapreduce}
MapReduce takes advantage of locality of data - by processing it on 
or near the storage assets in order to minimize network, space, IO, CPU cost on each machine.
Each round of MapReduce takes input in the form of $(key,value)$) pairs.
A single MapReduce round consists of three phases:

\begin{description}
	\item [Mapping] A function, or \textit{mapping}, is executed on each $(key,
		value)$ pair, resulting in a new set of $(key,value)$ pairs
	\item [Shuffling] The pairs emitted in the mapping phase are redistributed among
		the machines. Pairs are distributed so that every pair the
		same key is send to the same machine. We will rely on a stronger
		guarantee.
	\item [Reducing] A reducer, or \textit{fold}, is applied to each $(key,
		\{values\})$ pair.
\end{description}

Generally, the map and reduce functions are provided by the programmer, and the
framework handles shuffling. The TeraSort algorithm relying on the shuffling
phase to distribute keys so that the machines constitute a partial sort over the
keys. That is, for a sorted, random sample $samp$ over the keys, where $|samp|$ is equal to the
number of reducers $M$, all keys such that $samp_{i-1} \le key \le samp_i$ are send
to the $i$th reducer. Rather than imposing this
requirement on the framework, we can invent an operation
\textit[map-shuffle], that is allowed to decide which reducer it sends $(key,
value)$ pairs to after mapping. These two abstractions are equivalent for the
purpose of TeraSort \citep{minimalmapreduce}.

\subsection{Two subproblems}
Our problem of ranking movies based on averaged ratings
consist two subproblems. We need to to calculate the averages, and sort the
result.

\subsection{Calculating the Average Ratings}
We can calculate the averages in one MapReduce round.

The mapper simply emits the pairs in the input.

Each reducer then receives a list of $(j, \{r_1 \ldots r_n\})$ pairs: a movie id
$j$, and all ratings of it. The reducer then calculates the average rating for
each $j$, and emits a $(j, \bar{r})$ pair for each pair in the input.
This round does not rely on the special shuffle step as TeraSort does, but is
not hindered by it.

\subsection{TeraSort}
TeraSort can be thought of as a parallel, randomized quicksort with a cut-off to a
non-distributed sort.

The algorithm consists of two rounds of MapReduce.
\begin{description}
	\item[Round 1]
		Each mapper produces a random sample of it's inputs.

		Reducing all pairs on a single machine, the sample is sorted,
		and $b_1, \ldots, b_M-1$ elements are chosen from it, such that
		$b_i$ is the $\lceil |samp|/M \rceil $th smallest element in
		$samp$. Each $b_i$ is used as a partitioning element when the
		pairs are distributed in the next round.
	\item[Round 2]
		The \texttt{map-shuffle} step distributes it's inputs among the
		reducers, according to the partitioning elements produced in
		round 1.

		Each reducer uses a "normal" sort to sort it's inputs. Now, each
		reducers output is sorted internally, and we know the
		machine-ordering, since we know the partitioning elements, hence
		we know the total ordering of the data.

\end{description}

Together, the averaging and sorting requires three MapReduce rounds.

In paper Minimal MapReduce Algorithms TeraSort was classified as
minimal MapReduce algorithm. Which definition and properties are given
here:

\texttt{Tao et al. (2013)}\citep{minimalmapreduce}:

\begin{quote}
"Denote by $S$ the set of input objects 
for the underlying problem. Let n, the problem cardinality,
be the number of objects in $S$, and $t$ be the number of machines
used in the system. Define $m = n/t$, namely, $m$ is the number
of objects per machine when $S$ is evenly distributed across the
machines. Consider an algorithm for solving a problem on $S$.
We say that the algorithm is minimal if it has all of the following
properties.
\end{quote}

\begin{itemize}
\item \emph{Minimum footprint}: at all times, each machine uses only
O(m) space of storage.
\item \emph{Bounded net-traffic}: in each round, every machine sends
and receives at most $O(m)$ words of information over the
network.
\item  \emph{Constant round}: the algorithm must terminate after a
constant number of rounds.
\item \emph{optimal computation}: every machine performs only
$O(T_seq /t)$ amount of computation in total (i.e., summing
over all rounds), where $T_seq$ is the time needed to solve the
same problem on a single sequential machine. Namely, the
algorithm should achieve a speedup of t by using t machines
in parallel." 
\end{itemize}


\subsection{}
For good sampling rates in round 1, Terasort is minimal \texttt{Tao et al.
(2013)}\citep{minimalmapreduce}. It is easy to see that the first part ---
averging ratings --- is too, as no phases are super-linear in their input size,
and no phases produce an output larger than it's input.

The minimum footprint
property guarantees that no machine needs more than $(m)$ storagej
memory. Which means the memory consumption of each machine down-scales linearly
with the number of machines used. The same
hold for network throughput and processing time per machine
Using a constant number of rounds is an important property in the MapReduce
model. Shuffling is a very expensive operation, both in time and network
traffic.
