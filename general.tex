\section{General approach}
As mentioned in section \ref{sec:intro_real}, the simplest solution seems to be
ignoring the dynamic nature of the problem. We could simply add each input pair
$(j,r)$ to a list, at $O(1)$ cost per input, and then when queried:
\begin{enumerate}
	\item sort the list by movie $j$ at cost $O(|S| \log |S|)$
	\item calculate the averages at cost $O(|S|)$ (as the values relevant to
		each average are now adjacent)
	\item sort the resulting list by the averages at cost $O(|U| \log |U|)$
\end{enumerate}

This is obviously bad, as queries will take $O(|S| \log |S| + |U| \log|U|)$
time. Reassuming the dynamic approach, we can trivially improve this algorithm
by maintaining a hash table of running averages, updating the relevant value for
each input. Now queries are just $O(|U| \log |U|)$, and processing each input
element is $O(1)$. This is the lower bound for comparison based sorting, but we
may choose to prioritize a faster query time, in exchange for slower per-element
processing. This can be done by maintaining an always-ordered data structure of
$(j, \overline r)$ pairs, ordered by $\overline r$, being the running average of
ratings for $j$. While we can obviously do this using insertion sort, we can
improve on the resulting $O(U)$ per-element cost, without degrading the $O(U)$
query time it provides. This can be achieved by maintaining a binary search-tree
instead of a list, taking care not to delete nodes with equal average, if they
pertain to different movies, and to never keep more than one node for each
movie.

For efficient querying we need the tree to be ordered by the
running averages, but for updating the averages we need an efficient lookup
based on the movie id (the stored average cannot be known solely from the input
element). We additionally maintain a hash table, mapping movies to stored
averages or, equivalently, to pointers to nodes in the tree. This provides a
solution with $O(\log |U|)$ per-element processing time, $O(|U|)$ query
time, and $(|U|)$ memory consumption.

%\section{Heap and Min-Max Heap}
%
%From \texttt{Heap, Wikipedia}:
%\begin{quote}Heap is a specialized tree-based data structure that satisfies the heap property: 
%If A is a parent node of B then the key of node A is ordered with respect to the key of node 
%B with the same ordering applying across the heap.
%\end{quote}
%
%Ordering items using Heap takes $O(n\log{}n)$. Where creation of a heap takes
%$O(n)$. Then we are traversing through heap n times using fuction which takes
%$O(\log{}n)$. Which results in $O(n + n\log{}n)$, which leads to our prevoiusly
%stated complexity.
%
%Also Binary Heap can offer better average insertion time than BST which may
%be important factor in some scenarions when the data are often updated but we 
%still need to able to acess the sorting results.
%
%In scenario when we are only interested in Top and Low k elements of a set we might takes
%different heap based approach called Min-Max Heap.
%
%Provides properties of regular heap like a: 
%\begin{itemize}
%\item $O(n)$ built time,
%\item $O(\log{}n)$ for insertion and deletion
%\end{itemize}
%
%From \texttt{Min-max heap, Wikipedia}:
%\begin{quote}Heap is a specialized tree-based data structure that satisfies the heap property: 
%If A is a parent node of B then the key of node A is ordered with respect to the key of node 
%B with the same ordering applying across the heap.
%\end{quote}
%
%But even more important are properties carried by Min-Max variation. Which are 
%guaranteeing us a constant time for Min and Max operations. The Top/Low(k) 
%finding operation can be performed in $O(k\log{}k)$ time.
%
%Using this approach requires to store all input elements. It takes $O(n)$ for 
%large sets.
%
%Based on : \texttt{M. D. Atkinson, J.R. Sack, N. Santoro, and T. Strothotte, 
%Communications of the ACM, 
%October, 1986, 
%Min-Max Heaps and Generalized Priority Queues}
